{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>15.615220</td>\n",
       "      <td>15.678981</td>\n",
       "      <td>15.547723</td>\n",
       "      <td>15.610239</td>\n",
       "      <td>15.571998</td>\n",
       "      <td>78541293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>15.620949</td>\n",
       "      <td>15.637387</td>\n",
       "      <td>15.480475</td>\n",
       "      <td>15.541497</td>\n",
       "      <td>15.503423</td>\n",
       "      <td>120638494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>15.588072</td>\n",
       "      <td>15.588072</td>\n",
       "      <td>15.102393</td>\n",
       "      <td>15.149715</td>\n",
       "      <td>15.112601</td>\n",
       "      <td>159744526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>15.178109</td>\n",
       "      <td>15.193053</td>\n",
       "      <td>14.760922</td>\n",
       "      <td>14.797037</td>\n",
       "      <td>14.760787</td>\n",
       "      <td>257533695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>14.744733</td>\n",
       "      <td>15.024933</td>\n",
       "      <td>14.672753</td>\n",
       "      <td>14.994298</td>\n",
       "      <td>14.957565</td>\n",
       "      <td>189680313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3713</th>\n",
       "      <td>2024-10-04</td>\n",
       "      <td>169.339996</td>\n",
       "      <td>169.550003</td>\n",
       "      <td>166.960007</td>\n",
       "      <td>168.559998</td>\n",
       "      <td>168.559998</td>\n",
       "      <td>11422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>169.139999</td>\n",
       "      <td>169.899994</td>\n",
       "      <td>164.130005</td>\n",
       "      <td>164.389999</td>\n",
       "      <td>164.389999</td>\n",
       "      <td>14034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3715</th>\n",
       "      <td>2024-10-08</td>\n",
       "      <td>165.429993</td>\n",
       "      <td>166.100006</td>\n",
       "      <td>164.309998</td>\n",
       "      <td>165.699997</td>\n",
       "      <td>165.699997</td>\n",
       "      <td>11723900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3716</th>\n",
       "      <td>2024-10-09</td>\n",
       "      <td>164.854996</td>\n",
       "      <td>166.259995</td>\n",
       "      <td>161.119995</td>\n",
       "      <td>163.059998</td>\n",
       "      <td>163.059998</td>\n",
       "      <td>19649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3717</th>\n",
       "      <td>2024-10-10</td>\n",
       "      <td>162.110001</td>\n",
       "      <td>163.839996</td>\n",
       "      <td>161.649994</td>\n",
       "      <td>163.649994</td>\n",
       "      <td>163.649994</td>\n",
       "      <td>3999910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3718 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date        Open        High         Low       Close   Adj Close  \\\n",
       "0    2010-01-04   15.615220   15.678981   15.547723   15.610239   15.571998   \n",
       "1    2010-01-05   15.620949   15.637387   15.480475   15.541497   15.503423   \n",
       "2    2010-01-06   15.588072   15.588072   15.102393   15.149715   15.112601   \n",
       "3    2010-01-07   15.178109   15.193053   14.760922   14.797037   14.760787   \n",
       "4    2010-01-08   14.744733   15.024933   14.672753   14.994298   14.957565   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "3713 2024-10-04  169.339996  169.550003  166.960007  168.559998  168.559998   \n",
       "3714 2024-10-07  169.139999  169.899994  164.130005  164.389999  164.389999   \n",
       "3715 2024-10-08  165.429993  166.100006  164.309998  165.699997  165.699997   \n",
       "3716 2024-10-09  164.854996  166.259995  161.119995  163.059998  163.059998   \n",
       "3717 2024-10-10  162.110001  163.839996  161.649994  163.649994  163.649994   \n",
       "\n",
       "         Volume  \n",
       "0      78541293  \n",
       "1     120638494  \n",
       "2     159744526  \n",
       "3     257533695  \n",
       "4     189680313  \n",
       "...         ...  \n",
       "3713   11422100  \n",
       "3714   14034700  \n",
       "3715   11723900  \n",
       "3716   19649400  \n",
       "3717    3999910  \n",
       "\n",
       "[3718 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_symbol = \"GOOG\"\n",
    "start_date = \"2010-01-01\"\n",
    "end_date = pd.Timestamp.today()\n",
    "data = yf.download(stock_symbol, start = start_date, end = end_date)\n",
    "data.reset_index(inplace = True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape: (2974, 2)\n",
      "Testing Data shape: (744, 2)\n"
     ]
    }
   ],
   "source": [
    
    "# Step 1: Prepare the training and testing data\n",
    "data_train = pd.DataFrame({\n",
    "    'Open': data['Open'][0: int(len(data) * 0.8)],\n",
    "    'Close': data['Close'][0: int(len(data) * 0.8)]\n",
    "})\n",
    "\n",
    "data_test = pd.DataFrame({\n",
    "    'Open': data['Open'][int(len(data) * 0.8): len(data)],\n",
    "    'Close': data['Close'][int(len(data) * 0.8): len(data)]\n",
    "})\n",
    "\n",
    "print(\"Training Data shape:\", data_train.shape)\n",
    "print(\"Testing Data shape:\", data_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_train_scaled = scaler.fit_transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Prepare the input (x) and output (y) for the LSTM model\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# Create sequences of data\n",
    "time_steps = 60  # Number of time steps to look back\n",
    "\n",
    "for i in range(time_steps, data_train_scaled.shape[0]):\n",
    "    x.append(data_train_scaled[i - time_steps:i])  # Previous 60 time steps\n",
    "    y.append(data_train_scaled[i])  # Current Open and Close prices\n",
    "\n",
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Shreya\\Project\\stock prediction\\venv\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Build the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=50, activation='relu', return_sequences=True, input_shape=(x.shape[1], x.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(units=60, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(LSTM(units=80, activation='relu', return_sequences=True))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(LSTM(units=120, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output layer to predict both Open and Close prices\n",
    "model.add(Dense(units=2))  # 2 units for Open and Close prices\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 52ms/step - loss: 0.0323\n",
      "Epoch 2/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0054\n",
      "Epoch 3/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0037\n",
      "Epoch 4/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0032\n",
      "Epoch 5/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0031\n",
      "Epoch 6/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0028\n",
      "Epoch 7/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0026\n",
      "Epoch 8/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0023\n",
      "Epoch 9/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 0.0029\n",
      "Epoch 10/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0018\n",
      "Epoch 11/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0020\n",
      "Epoch 12/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0019\n",
      "Epoch 13/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0017\n",
      "Epoch 14/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0020\n",
      "Epoch 15/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0023\n",
      "Epoch 16/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0020\n",
      "Epoch 17/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0018\n",
      "Epoch 18/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 0.0017\n",
      "Epoch 19/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0018\n",
      "Epoch 20/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0017\n",
      "Epoch 21/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 22/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0019\n",
      "Epoch 23/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0015\n",
      "Epoch 24/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 0.0015\n",
      "Epoch 25/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0014\n",
      "Epoch 26/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 0.0020\n",
      "Epoch 27/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0018\n",
      "Epoch 28/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0019\n",
      "Epoch 29/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0017\n",
      "Epoch 30/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 31/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 32/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0018\n",
      "Epoch 33/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 34/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0014\n",
      "Epoch 35/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0015\n",
      "Epoch 36/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0015\n",
      "Epoch 37/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0015\n",
      "Epoch 38/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0014\n",
      "Epoch 39/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - loss: 0.0014\n",
      "Epoch 40/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 0.0014\n",
      "Epoch 41/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0015\n",
      "Epoch 42/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 43/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0016\n",
      "Epoch 44/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0016\n",
      "Epoch 45/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 46/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0012\n",
      "Epoch 47/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0015\n",
      "Epoch 48/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 49/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0015\n",
      "Epoch 50/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0017\n",
      "Epoch 51/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0015\n",
      "Epoch 52/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 53/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 54/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 55/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 56/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0013\n",
      "Epoch 57/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0016\n",
      "Epoch 58/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0013\n",
      "Epoch 59/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0012\n",
      "Epoch 60/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0021\n",
      "Epoch 61/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 62/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0013\n",
      "Epoch 63/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0015\n",
      "Epoch 64/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0013\n",
      "Epoch 65/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0014\n",
      "Epoch 66/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 49ms/step - loss: 0.0016\n",
      "Epoch 67/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0012\n",
      "Epoch 68/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0014\n",
      "Epoch 69/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 70/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 71/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - loss: 0.0016\n",
      "Epoch 72/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 73/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0015\n",
      "Epoch 74/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0015\n",
      "Epoch 75/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0011\n",
      "Epoch 76/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0014\n",
      "Epoch 77/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0014\n",
      "Epoch 78/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0014\n",
      "Epoch 79/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0012\n",
      "Epoch 80/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0014\n",
      "Epoch 81/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0013\n",
      "Epoch 82/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0014\n",
      "Epoch 83/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0016\n",
      "Epoch 84/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 85/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0013\n",
      "Epoch 86/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0016\n",
      "Epoch 87/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 88/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0015\n",
      "Epoch 89/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0013\n",
      "Epoch 90/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0013\n",
      "Epoch 91/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0013\n",
      "Epoch 92/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 54ms/step - loss: 0.0014\n",
      "Epoch 93/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 53ms/step - loss: 0.0013\n",
      "Epoch 94/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 95/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0016\n",
      "Epoch 96/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 97/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 52ms/step - loss: 0.0014\n",
      "Epoch 98/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0013\n",
      "Epoch 99/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0013\n",
      "Epoch 100/100\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 51ms/step - loss: 0.0012\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1da3595f340>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Train the model\n",
    "model.fit(x, y, epochs=100, batch_size=32)  # Adjust epochs and batch size as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Prepare the test data (you can do this in a similar way)\n",
    "data_test_scaled = scaler.transform(data_test)\n",
    "\n",
    "x_test = []\n",
    "for i in range(time_steps, data_test_scaled.shape[0]):\n",
    "    x_test.append(data_test_scaled[i - time_steps:i])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], x_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Make predictions\n",
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Inverse transform the predictions to get actual prices\n",
    "predictions_inverse = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions_inverse will contain two columns: [Predicted Open, Predicted Close]\n",
    "predicted_open_price = predictions_inverse[:, 0]\n",
    "predicted_close_price = predictions_inverse[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Open Prices: [140.70297  140.27325  139.57759  138.623    137.5402   136.51387\n",
      " 135.69823  135.23654  135.21317  135.74797  136.73883  137.87112\n",
      " 138.87471  139.54027  139.82574  139.80481  139.51598  139.01628\n",
      " 138.43529  137.93892  137.56609  137.24739  136.89342  136.4464\n",
      " 135.92065  135.45432  135.1794   135.1146   135.22498  135.4425\n",
      " 135.64902  135.6637   135.3466   134.81865  134.28824  133.85905\n",
      " 133.51422  133.20422  132.99728  132.98232  133.19788  133.62672\n",
      " 134.22887  134.92612  135.64577  136.35095  137.00037  137.58327\n",
      " 138.10458  138.52444  138.80627  138.9903   139.14488  139.24207\n",
      " 139.20963  139.01929  138.62769  138.01735  137.25471  136.41959\n",
      " 135.58102  134.841    134.28015  133.85545  133.38222  132.7152\n",
      " 131.82056  130.59943  129.14548  127.59835  126.06308  124.69644\n",
      " 123.69992  123.08831  122.68569  122.249146 121.69592  120.99924\n",
      " 120.14516  119.27392  118.500404 117.937355 117.53448  117.131035\n",
      " 116.602745 115.94305  115.056725 113.92693  112.69509  111.65139\n",
      " 111.03476  110.964165 111.45426  112.32576  113.39371  114.45398\n",
      " 115.36038  115.98035  116.146164 115.682625 114.66129  113.37759\n",
      " 112.03897  110.81871  109.93697  109.532555 109.6487   110.31902\n",
      " 111.45674  112.73854  113.80055  114.355034 114.28542  113.75243\n",
      " 113.097664 112.66663  112.6954   113.17626  113.912056 114.57481\n",
      " 114.908104 114.88471  114.544174 114.06081  113.6275   113.388405\n",
      " 113.24064  113.031715 112.60894  112.1277   111.793945 111.775154\n",
      " 112.11523  112.72877  113.54731  114.49068  115.414406 116.22138\n",
      " 116.82966  117.28575  117.669    118.0741   118.563576 119.13364\n",
      " 119.68425  120.131195 120.37528  120.28915  119.837555 119.10552\n",
      " 118.30491  117.51899  116.715836 115.895836 115.09412  114.355286\n",
      " 113.71425  113.125694 112.606514 112.17556  111.889946 111.79475\n",
      " 111.72743  111.51229  111.02741  110.242096 109.23248  108.10365\n",
      " 106.942924 105.819695 104.78483  103.85819  103.06326  102.44169\n",
      " 101.97843  101.58332  101.23765  101.05202  101.08138  101.31569\n",
      " 101.62272  101.84862  101.86965  101.65533  101.24377  100.72976\n",
      " 100.263855 100.03317  100.06027  100.28866  100.60965  101.000175\n",
      " 101.486946 101.77118  101.53583  100.77844   99.7083    98.46128\n",
      "  97.02517   95.28898   93.44325   91.778824  90.50572   89.664925\n",
      "  89.379     89.73042   90.66498   92.09084   93.797424  95.499054\n",
      "  96.96609   97.98973   98.495926  98.60445   98.463615  98.16487\n",
      "  97.76552   97.420044  97.32577   97.548134  98.02567   98.584526\n",
      "  98.99277   99.07628   98.74079   98.04061   97.27638   96.596756\n",
      "  96.01253   95.46879   94.90082   94.26325   93.60423   92.94045\n",
      "  92.327225  91.79457   91.30836   90.89459   90.57314   90.4051\n",
      "  90.4122    90.49454   90.56926   90.61946   90.62062   90.67259\n",
      "  90.86115   91.214386  91.69989   92.24079   92.77104   93.39897\n",
      "  94.25388   95.30885   96.36009   97.282364  98.03985   98.56972\n",
      "  98.876144  99.05288   99.41137  100.02585  100.787994 101.65513\n",
      " 102.39111  102.70965  102.39723  101.47963  100.19484   98.88051\n",
      "  97.71002   96.7838    96.08608   95.5543    95.1265    94.703255\n",
      "  94.251945  93.78056   93.33197   92.97728   92.82897   92.98325\n",
      "  93.425995  94.051994  94.686584  95.12313   95.2169    95.07464\n",
      "  94.87273   94.84905   95.2176    96.0258    97.25287   98.75604\n",
      " 100.37438  101.916046 103.15664  103.90635  104.13182  103.89865\n",
      " 103.40493  102.88066  102.535675 102.498344 102.8541   103.54862\n",
      " 104.41441  105.26316  105.999    106.612305 107.02427  107.19752\n",
      " 107.11632  106.84467  106.49837  106.2006   106.0003   105.890785\n",
      " 105.915985 106.11652  106.46597  106.86486  107.202965 107.39449\n",
      " 107.41887  107.34445  107.27979  107.37326  107.853546 108.83122\n",
      " 110.185616 111.74337  113.37652  114.9937   116.52114  117.92504\n",
      " 119.135574 120.03822  120.68288  121.16691  121.588    121.95968\n",
      " 122.29258  122.62935  123.01859  123.517944 124.06206  124.49335\n",
      " 124.72401  124.78604  124.779594 124.766884 124.80475  124.9255\n",
      " 125.080605 125.16906  125.14823  125.02856  124.77104  124.33069\n",
      " 123.76739  123.18651  122.71045  122.39594  122.27423  122.30348\n",
      " 122.401024 122.41056  122.22546  121.90137  121.65834  121.69066\n",
      " 122.06277  122.67563  123.35192  123.84819  124.02426  123.91488\n",
      " 123.6423   123.54786  123.871216 124.67561  125.86956  127.20712\n",
      " 128.39352  129.22748  129.6653   129.81886  129.84961  129.87401\n",
      " 129.9589   130.09921  130.29501  130.53767  130.76118  130.94269\n",
      " 131.04434  131.03897  130.96605  130.94331  131.05266  131.2555\n",
      " 131.5139   131.85614  132.32484  132.92088  133.59534  134.25548\n",
      " 134.8086   135.19801  135.43575  135.5912   135.70944  135.81583\n",
      " 135.96811  136.20004  136.49376  136.81766  137.10162  137.19746\n",
      " 137.00815  136.5774   135.98206  135.3237   134.7473   134.36365\n",
      " 134.22829  134.32512  134.59741  134.98174  135.42     135.88411\n",
      " 136.35278  136.81853  137.27742  137.682    137.99751  138.23761\n",
      " 138.41273  138.5249   138.55986  138.50482  138.42538  138.21542\n",
      " 137.61285  136.59915  135.3278   134.01884  132.87233  132.06198\n",
      " 131.6781   131.70377  132.047    132.57446  133.1276   133.5894\n",
      " 133.91092  134.1238   134.30093  134.518    134.79984  135.11368\n",
      " 135.44734  135.81822  136.21051  136.56999  136.86293  137.0736\n",
      " 137.15517  137.05702  136.74565  136.25569  135.7059   135.30699\n",
      " 135.19814  135.31735  135.53545  135.75034  135.90088  135.9467\n",
      " 135.92621  135.94429  136.09865  136.44106  136.9609   137.5961\n",
      " 138.2459   138.81021  139.2222   139.4416   139.47528  139.3727\n",
      " 139.17973  138.97426  138.87148  138.95622  139.25209  139.71112\n",
      " 140.23851  140.72743  141.11368  141.42941  141.74165  142.07504\n",
      " 142.44298  142.87784  143.39229  143.95476  144.52634  145.00766\n",
      " 145.15273  145.001    144.62592  144.2043   143.86507  143.69856\n",
      " 143.73491  143.9654   144.29138  144.5818   144.78552  144.79973\n",
      " 144.60977  144.25903  143.88127  143.59912  143.4177   143.2411\n",
      " 143.0194   142.71541  142.36702  141.96141  141.44139  140.80406\n",
      " 140.122    139.50932  139.07509  138.88612  138.937    139.21042\n",
      " 139.64314  140.18393  140.83241  141.50185  142.1379   142.69858\n",
      " 143.18913  143.60391  143.96132  144.28165  144.58774  144.93408\n",
      " 145.32498  145.74274  146.10588  146.3909   146.63971  146.91364\n",
      " 147.20764  147.55502  147.91788  148.25378  148.48396  148.63547\n",
      " 148.72739  148.7737   148.80412  148.88364  149.05637  149.08626\n",
      " 149.41364  149.83601  150.29488  150.6782   150.9764   151.20445\n",
      " 151.3509   151.50731  151.6474   151.82397  152.00513  152.10915\n",
      " 152.18988  152.23563  152.288    152.35155  152.43825  152.5809\n",
      " 152.76353  152.92639  153.03873  153.11295  153.20312  153.27577\n",
      " 153.31284  153.31873  153.26323  153.19083  153.1621   153.16908\n",
      " 153.16792  153.14804  153.20886  153.24736  153.27351  153.29916\n",
      " 153.3111   153.25862  153.16965  153.00766  152.85179  152.68732\n",
      " 152.64738  152.63046  152.6147   152.54596  152.38321  152.1953\n",
      " 151.99075  151.77394  151.6869   151.64618  151.59717  151.57\n",
      " 151.6284   151.72023  151.8276   151.79286  151.75539  151.99365\n",
      " 152.23766  152.5766   152.7667   152.92355  152.95618  153.03777\n",
      " 153.20233  153.62062  153.86604  154.07999  154.16473  154.19034\n",
      " 154.12338  154.02585  154.00232  153.98544  153.9544   153.83974\n",
      " 153.72292  153.65501  153.61308  153.58731  153.54109  153.51854\n",
      " 153.5034   153.48     153.42061  153.41075  153.37552  153.27103\n",
      " 153.16081  153.01018  152.77098  152.43684  152.01837  151.6156\n",
      " 151.30652  151.119    151.06035  151.0611   151.14163  151.2723\n",
      " 151.40694  151.52405  151.55891  151.55574  151.50966  151.45645\n",
      " 151.48622  151.5784   151.66954  151.83716  151.9855   152.124   ]\n",
      "Predicted Close Prices: [140.78899  140.35898  139.66287  138.70766  137.62419  136.59721\n",
      " 135.7811   135.31915  135.29584  135.83107  136.82271  137.95592\n",
      " 138.96039  139.62653  139.91228  139.89131  139.60222  139.10216\n",
      " 138.52072  138.02394  137.65079  137.33186  136.97765  136.53033\n",
      " 136.00429  135.53766  135.26257  135.19774  135.30821  135.52592\n",
      " 135.73264  135.74734  135.43011  134.90187  134.37117  133.94174\n",
      " 133.5967   133.28654  133.07951  133.06454  133.28027  133.7094\n",
      " 134.31197  135.00967  135.72984  136.43553  137.08539  137.6687\n",
      " 138.19034  138.61046  138.89243  139.07652  139.23119  139.32835\n",
      " 139.2958   139.10524  138.71329  138.10248  137.33928  136.50359\n",
      " 135.66447  134.92397  134.36276  133.93784  133.46432  132.79701\n",
      " 131.90193  130.6802   129.22548  127.67753  126.14142  124.774025\n",
      " 123.77698  123.16507  122.762314 122.32564  121.7722   121.07519\n",
      " 120.220665 119.34894  118.574974 118.01157  117.60847  117.2048\n",
      " 116.67622  116.01612  115.12922  113.99862  112.765854 111.72129\n",
      " 111.10404  111.03326  111.523605 112.3957   113.46449  114.525665\n",
      " 115.43287  116.05348  116.21959  115.7559   114.73391  113.44928\n",
      " 112.109604 110.88828  110.00569  109.600746 109.71676  110.38743\n",
      " 111.52593  112.80872  113.87166  114.426765 114.35731  113.824066\n",
      " 113.16888  112.737495 112.7662   113.247345 113.98365  114.64696\n",
      " 114.980606 114.95732  114.61661  114.13294  113.69931  113.45999\n",
      " 113.312065 113.10296  112.67985  112.19822  111.86414  111.84526\n",
      " 112.18548  112.79943  113.618576 114.56267  115.48716  116.29484\n",
      " 116.903694 117.36024  117.74384  118.14927  118.63914  119.20962\n",
      " 119.760635 120.20791  120.45221  120.36606  119.91421  119.18167\n",
      " 118.38046  117.59394  116.790146 115.96948  115.16712  114.427666\n",
      " 113.78607  113.19701  112.67735  112.246    111.96008  111.864746\n",
      " 111.79732  111.582    111.096756 110.31081  109.30035  108.17054\n",
      " 107.00874  105.88445  104.848526 103.92091  103.1251   102.50277\n",
      " 102.03889  101.64325  101.29712  101.11115  101.14039  101.374725\n",
      " 101.68192  101.907974 101.92905  101.714584 101.30273  100.7883\n",
      " 100.32197  100.090996 100.118    100.34649  100.66768  101.058495\n",
      " 101.54568  101.83022  101.59481  100.836914  99.76598   98.51796\n",
      "  97.080605  95.3428    93.495316  91.82936   90.555115  89.713524\n",
      "  89.42718   89.77864   90.7137    92.14053   93.84843   95.55148\n",
      "  97.01986   98.04459   98.55146   98.66031   98.51955   98.22073\n",
      "  97.82116   97.475426  97.38102   97.603455  98.08127   98.64051\n",
      "  99.04911   99.132774  98.797134  98.09649   97.33173   96.65157\n",
      "  96.06685   95.52265   94.95419   94.31608   93.656494  92.99211\n",
      "  92.37833   91.8452    91.358536  90.944374  90.62259   90.45432\n",
      "  90.46132   90.54364   90.61838   90.66862   90.66976   90.721756\n",
      "  90.91044   91.26389   91.74973   92.29104   92.82172   93.45015\n",
      "  94.305725  95.36153   96.413635  97.33674   98.09497   98.62542\n",
      "  98.93224   99.10927   99.46808  100.08307  100.845856 101.71377\n",
      " 102.4505   102.76946  102.457016 101.538795 100.25302   98.93768\n",
      "  97.76626   96.8392    96.1408    95.608444  95.18018   94.75651\n",
      "  94.30476   93.832954  93.38395   93.028885  92.88036   93.0346\n",
      "  93.477516  94.1039    94.73892   95.17587   95.26981   95.12752\n",
      "  94.92552   94.90183   95.27061   96.07934   97.3073    98.811676\n",
      " 100.43141  101.97449  103.21634  103.96699  104.19294  103.95984\n",
      " 103.46589  102.94127  102.595985 102.55855  102.91451  103.609535\n",
      " 104.47605  105.32556  106.062126 106.676094 107.088554 107.26213\n",
      " 107.18103  106.909294 106.56278  106.264786 106.06431  105.95469\n",
      " 105.97986  106.18051  106.53021  106.929405 107.267815 107.459564\n",
      " 107.48403  107.40962  107.34495  107.43848  107.9191   108.8975\n",
      " 110.252975 111.81203  113.44662  115.065254 116.59411  117.99929\n",
      " 119.21092  120.11443  120.75969  121.24419  121.66562  122.03759\n",
      " 122.37072  122.70768  123.09713  123.59672  124.14111  124.57263\n",
      " 124.80342  124.86546  124.85899  124.84623  124.88407  125.004845\n",
      " 125.15999  125.24848  125.22762  125.10788  124.8502   124.4096\n",
      " 123.84596  123.264694 122.7883   122.47358  122.35177  122.38104\n",
      " 122.478645 122.488235 122.30306  121.9788   121.73563  121.76795\n",
      " 122.14025  122.75344  123.430145 123.92677  124.103004 123.993614\n",
      " 123.72092  123.6264   123.949875 124.754654 125.94923  127.28754\n",
      " 128.47464  129.30913  129.74721  129.90085  129.93156  129.95589\n",
      " 130.04074  130.18106  130.37688  130.61963  130.84319  131.02475\n",
      " 131.12643  131.12103  131.04803  131.02524  131.1346   131.33752\n",
      " 131.59604  131.93845  132.40738  133.00374  133.6786   134.33914\n",
      " 134.89256  135.28221  135.52007  135.67555  135.79381  135.90019\n",
      " 136.0525   136.28452  136.57838  136.90247  137.1866   137.28242\n",
      " 137.09296  136.6619   136.06612  135.4073   134.83052  134.44661\n",
      " 134.31114  134.40804  134.68051  135.06511  135.50366  135.96811\n",
      " 136.43712  136.90318  137.36241  137.76727  138.083    138.3232\n",
      " 138.49843  138.61063  138.64554  138.59041  138.51088  138.30074\n",
      " 137.69772  136.68335  135.41118  134.10143  132.95424  132.14337\n",
      " 131.7593   131.78502  132.12851  132.65637  133.20992  133.6721\n",
      " 133.99388  134.20694  134.38422  134.60146  134.88345  135.1975\n",
      " 135.53134  135.90245  136.295    136.6547   136.94778  137.15858\n",
      " 137.24016  137.14192  136.83032  136.33997  135.7898   135.39058\n",
      " 135.2816   135.40088  135.6191   135.83415  135.98479  136.03067\n",
      " 136.01018  136.02827  136.18271  136.52534  137.04556  137.68118\n",
      " 138.33147  138.8962   139.30844  139.52802  139.56166  139.45894\n",
      " 139.26576  139.0601   138.95714  139.04192  139.33795  139.7973\n",
      " 140.3251   140.81439  141.20093  141.51689  141.82935  142.16296\n",
      " 142.53114  142.96637  143.4812   144.04417  144.61624  145.09789\n",
      " 145.24295  145.09099  144.71541  144.2933   143.95369  143.78696\n",
      " 143.82332  144.05399  144.3803   144.67097  144.87485  144.88902\n",
      " 144.69885  144.34776  143.9696   143.68718  143.5056   143.32881\n",
      " 143.10692  142.80272  142.45406  142.04813  141.52773  140.88997\n",
      " 140.20741  139.59436  139.15985  138.97083  139.02179  139.29552\n",
      " 139.72864  140.26996  140.91902  141.58908  142.22571  142.7869\n",
      " 143.27788  143.69298  144.05069  144.37123  144.67758  145.02419\n",
      " 145.4154   145.83354  146.19691  146.48218  146.7312   147.00536\n",
      " 147.29968  147.64738  148.01062  148.3468   148.57718  148.7287\n",
      " 148.82066  148.86691  148.89732  148.9769   149.14978  149.17995\n",
      " 149.50793  149.93105  150.39062  150.77455  151.07318  151.30159\n",
      " 151.44832  151.60501  151.7454   151.9223   152.10371  152.20793\n",
      " 152.2889   152.3349   152.38763  152.45166  152.53893  152.68214\n",
      " 152.86531  153.02853  153.14108  153.21542  153.3056   153.37819\n",
      " 153.41516  153.42093  153.36531  153.29291  153.2643   153.27145\n",
      " 153.27046  153.2508   153.31177  153.3504   153.3766   153.40227\n",
      " 153.41418  153.36168  153.27274  153.11093  152.95534  152.79132\n",
      " 152.7519   152.73544  152.72005  152.65152  152.48904  152.3014\n",
      " 152.09721  151.88081  151.79417  151.75368  151.70483  151.67764\n",
      " 151.73592  151.82748  151.93448  151.89926  151.86139  152.09932\n",
      " 152.34282  152.68114  152.87044  153.02655  153.05855  153.13968\n",
      " 153.30386  153.72171  153.96646  154.17969  154.26367  154.2886\n",
      " 154.22112  154.12323  154.09949  154.0825   154.05138  153.9366\n",
      " 153.8198   153.75204  153.71024  153.68459  153.63847  153.61598\n",
      " 153.60086  153.5774   153.5179   153.50787  153.47237  153.36748\n",
      " 153.25679  153.10562  152.86578  152.53098  152.11188  151.70871\n",
      " 151.39946  151.21202  151.15367  151.15485  151.2359   151.36707\n",
      " 151.50215  151.61957  151.6546   151.6515   151.6055   151.55241\n",
      " 151.58244  151.67493  151.76642  151.93439  152.08298  152.22159 ]\n"
     ]
    }
   ],
   "source": [
    "# Output the predicted prices\n",
    "print(\"Predicted Open Prices:\", predicted_open_price)\n",
    "print(\"Predicted Close Prices:\", predicted_close_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('stock_price_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
